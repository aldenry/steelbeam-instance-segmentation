{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fad22b1",
   "metadata": {},
   "source": [
    "# 1. Theoretical Background\n",
    "\n",
    "---\n",
    "\n",
    "## A. Object Detection (Bounding Box) vs Instance Segmentation (Pixel Mask)\n",
    "\n",
    "In early Computer Vision applications, the most widely used method was **Object Detection**, where the model predicts:\n",
    "\n",
    "1. The **location** of an object using a bounding box, and  \n",
    "2. The **class label** of that object.\n",
    "\n",
    "A bounding box is only an approximate area—it is always rectangular and does not follow the true contour of the object.  \n",
    "This approach works well for simple objects, but becomes limiting when dealing with shapes that are thin, irregular, or overlapping, such as steel beam profiles.\n",
    "\n",
    "To overcome this, we use **Instance Segmentation**, which predicts **pixel-level masks** that match the real shape of each object.\n",
    "\n",
    "| Method | What It Predicts | Strengths | Limitations |\n",
    "|--------|------------------|-----------|-------------|\n",
    "| Object Detection | Bounding box + label | Fast and efficient | Cannot capture exact object shape |\n",
    "| Instance Segmentation | Pixel mask + label | Highly detailed and accurate | Heavier than detection |\n",
    "\n",
    "In short, segmentation allows the model not only to *locate* the object but also to *trace its outline*.\n",
    "\n",
    "---\n",
    "\n",
    "## B. How Models Predict Segmentation Masks\n",
    "\n",
    "Models such as **YOLOv8-Seg** create masks through a structured set of steps rather than drawing them directly:\n",
    "\n",
    "1. **Feature Extraction**  \n",
    "   The image is passed through a backbone network (e.g., CSPDarknet), where the model learns patterns such as edges, corners, textures, and shapes.\n",
    "\n",
    "2. **Multi-scale Feature Fusion**  \n",
    "   Features from different depths of the network are combined.  \n",
    "   This helps the model understand both fine details and broader context.\n",
    "\n",
    "3. **Prototype Mask Generation**  \n",
    "   YOLOv8 produces a set of base masks called **prototypes**, which act like “universal components” the model can reuse.\n",
    "\n",
    "4. **Mask Coefficient Prediction**  \n",
    "   For each detected object, the model predicts coefficients that determine how to combine these prototypes.\n",
    "\n",
    "5. **Constructing the Final Mask**  \n",
    "   The final mask is produced by mixing prototype masks using those coefficients.  \n",
    "   This allows precise boundaries even when objects overlap or have complex shapes—common in steel beam imagery.\n",
    "\n",
    "In essence:  \n",
    "**The model does not paint masks from scratch; it assembles them by blending learned prototype shapes.**\n",
    "\n",
    "---\n",
    "\n",
    "## C. Transfer Learning in Segmentation\n",
    "\n",
    "Training a segmentation model from scratch would require massive datasets and very long training time.  \n",
    "To make the process efficient, we use **transfer learning**.\n",
    "\n",
    "The base model (e.g., YOLOv8-Seg) is pre-trained on a large dataset like **COCO**, so it already understands general visual patterns, including:\n",
    "\n",
    "- edges and contours  \n",
    "- common object shapes  \n",
    "- texture differences  \n",
    "- how to distinguish foreground and background  \n",
    "\n",
    "When we fine-tune the model on a steel-beam dataset, we are essentially adapting this prior knowledge to recognize new, domain-specific shapes such as:\n",
    "\n",
    "- I-beam  \n",
    "- L-beam  \n",
    "- O-beam  \n",
    "- O-pipe  \n",
    "- T-beam  \n",
    "- square-bar  \n",
    "- square-pipe  \n",
    "\n",
    "### Benefits of Transfer Learning\n",
    "- Dramatically faster training  \n",
    "- Requires fewer labeled images  \n",
    "- More stable learning from early epochs  \n",
    "- Produces cleaner, more accurate masks  \n",
    "\n",
    "After fine-tuning, the model becomes specialized and can reliably segment steel-beam profiles in new images.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31356eaa",
   "metadata": {},
   "source": [
    "## 2. Project Implementation\n",
    "\n",
    "### 2.1 Dataset Setup and Preprocessing\n",
    "\n",
    "In this project, I use the **steel beam instance segmentation** dataset from Roboflow, exported in the **YOLOv8 format**.  \n",
    "After extraction, the folder structure becomes:\n",
    "\n",
    "- `datasets/yolov8data/train/images`  \n",
    "- `datasets/yolov8data/train/labels`  \n",
    "- `datasets/yolov8data/valid/images`  \n",
    "- `datasets/yolov8data/valid/labels`  \n",
    "- `datasets/yolov8data/data.yaml`\n",
    "\n",
    "YOLOv8 automatically handles the **image resizing** and **augmentation** based on the `imgsz` parameter and training configuration.\n",
    "\n",
    "A few important preprocessing notes:\n",
    "\n",
    "- All images are resized to a fixed resolution (for example `320 × 320`) during training.  \n",
    "- Mask labels stored in YOLOv8 polygon `.txt` format are resized consistently along with the images.  \n",
    "- Augmentations used include horizontal flips, light brightness/contrast adjustments, and random cropping provided by YOLOv8.\n",
    "\n",
    "The following section contains code to confirm that the dataset is correctly loaded and to display sample images along with their annotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23acadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "DATA_YAML = \"../datasets/yolov8data/data.yaml\"\n",
    "\n",
    "data_path = Path(\"../datasets/yolov8data\")\n",
    "print(\"Ada data.yaml? \", (data_path / \"data.yaml\").exists())\n",
    "print(\"Folder train images: \", (data_path / \"train\" / \"images\").exists())\n",
    "print(\"Folder valid images: \", (data_path / \"valid\" / \"images\").exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19d99e1",
   "metadata": {},
   "source": [
    "Example of displaying one training image along with its label (optional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a9f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_img_dir = data_path / \"train\" / \"images\"\n",
    "sample_img = random.choice(list(train_img_dir.glob(\"*.jpg\")))\n",
    "print(\"Contoh gambar:\", sample_img.name)\n",
    "\n",
    "img = cv2.imread(str(sample_img))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Contoh Gambar Train\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e29f6",
   "metadata": {},
   "source": [
    "### 2.2 Model Training\n",
    "\n",
    "For this project, I used **YOLOv8-seg (n version / yolov8n-seg)** as the initial backbone.  \n",
    "The reasons are:\n",
    "\n",
    "- The available GPU only has 4 GB of VRAM, so the lightweight `n` variant is safer for both training and inference.\n",
    "- Training time is shorter while still allowing the model to learn patterns from all 7 steel profile classes.\n",
    "\n",
    "Main training configuration (modifiable):\n",
    "\n",
    "- `imgsz` : 320  \n",
    "- `epochs`: 40  \n",
    "- `batch` : 4  \n",
    "- `optimizer`: default YOLO  \n",
    "- `device`: CUDA  \n",
    "- `data`   : `data.yaml` from the dataset  \n",
    "\n",
    "Below is the training code snippet used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "DATA_YAML = \"../datasets/yolov8data/data.yaml\"\n",
    "\n",
    "model = YOLO(\"yolov8n-seg.pt\") \n",
    "\n",
    "results_train = model.train(\n",
    "    data=DATA_YAML,\n",
    "    imgsz=320,\n",
    "    epochs=50,\n",
    "    batch=4,\n",
    "    lr0=1e-3,\n",
    "    patience=10,\n",
    "    device=device,\n",
    "    workers=0,\n",
    "    name=\"steelbeam-nseg-320-b4-e50\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9607c071",
   "metadata": {},
   "source": [
    "### 2.3 Evaluation: mAP and IoU\n",
    "\n",
    "The metrics used in this project are not simple accuracy, but:\n",
    "\n",
    "- **mAP@0.5** (mean Average Precision at an IoU threshold of 0.5)  \n",
    "- **mAP@0.5:0.95** (the average mAP across multiple IoU thresholds)  \n",
    "- In addition, I also inspected the IoU values and visually checked the mask quality on the validation data.\n",
    "\n",
    "The evaluation was performed using the `model.val()` command provided by YOLOv8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7071b9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.235  Python-3.11.9 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 4096MiB)\n",
      "YOLOv8n-seg summary (fused): 85 layers, 3,259,429 parameters, 0 gradients, 11.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 483.976.9 MB/s, size: 65.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\UNSRI\\GDG\\steelbeam\\datasets\\yolov8data\\valid\\labels.cache... 262 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 262/262 262.5Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\UNSRI\\GDG\\steelbeam\\datasets\\yolov8data\\valid\\images\\2A7090FC_jpg.rf.be4d0ed5f0f4badad56da7ddbd00143d.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\UNSRI\\GDG\\steelbeam\\datasets\\yolov8data\\valid\\images\\452A534A_jpg.rf.d2b7dd6388e9652e81eea65b47555e78.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\UNSRI\\GDG\\steelbeam\\datasets\\yolov8data\\valid\\images\\IMG-20200513-WA0064_jpg.rf.6aa04ae4601908f1c91f8a5c59b97ad0.jpg: 10 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\UNSRI\\GDG\\steelbeam\\datasets\\yolov8data\\valid\\images\\IMG-20200513-WA0064_jpg.rf.d7f30ac4f53d08997eb9a8b2b54e244e.jpg: 10 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\UNSRI\\GDG\\steelbeam\\datasets\\yolov8data\\valid\\images\\IMG-20200513-WA0069_jpg.rf.218eadbdaa725bd47400312385f1dbbe.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\UNSRI\\GDG\\steelbeam\\datasets\\yolov8data\\valid\\images\\IMG-20200513-WA0069_jpg.rf.5dbaa125f86f96324a161464e3098bb0.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\UNSRI\\GDG\\steelbeam\\datasets\\yolov8data\\valid\\images\\IMG-20200515-WA0009_jpg.rf.32a7a3c6129aecc7305b3cae6d365999.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\UNSRI\\GDG\\steelbeam\\datasets\\yolov8data\\valid\\images\\IMG-20200515-WA0009_jpg.rf.c234dd106ae7c51fb99face3d45fe628.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\UNSRI\\GDG\\steelbeam\\datasets\\yolov8data\\valid\\images\\IMG-20200515-WA0035_jpg.rf.27ef5b7b627d0cd1994c6d33234f7f06.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\UNSRI\\GDG\\steelbeam\\datasets\\yolov8data\\valid\\images\\IMG-20200515-WA0035_jpg.rf.6a94c8444a0534db8145fb962f8c9b4f.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\UNSRI\\GDG\\steelbeam\\datasets\\yolov8data\\valid\\images\\IMG-20200515-WA0060_jpg.rf.02bfe2045456a12ea83921275042418c.jpg: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\UNSRI\\GDG\\steelbeam\\datasets\\yolov8data\\valid\\images\\IMG-20200515-WA0060_jpg.rf.a360d7492a9c38641f5e956207fbe94a.jpg: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\UNSRI\\GDG\\steelbeam\\datasets\\yolov8data\\valid\\images\\IMG-20200515-WA0108_jpg.rf.8b1cb715e2e1a3ddebc9cfe6c4233aa6.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\UNSRI\\GDG\\steelbeam\\datasets\\yolov8data\\valid\\images\\IMG-20200515-WA0108_jpg.rf.ac88cf6339a2deada84962b098043155.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\UNSRI\\GDG\\steelbeam\\datasets\\yolov8data\\valid\\images\\IMG-20200515-WA0116_jpg.rf.16aeac069945bed6fa59641b1d684913.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\UNSRI\\GDG\\steelbeam\\datasets\\yolov8data\\valid\\images\\IMG-20200515-WA0116_jpg.rf.afdeb98c9183af16c42dafc8a2566bfe.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\UNSRI\\GDG\\steelbeam\\datasets\\yolov8data\\valid\\images\\IMG-20200722-WA0008_jpg.rf.40325fd55bed3f4e727ff68207b65152.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\UNSRI\\GDG\\steelbeam\\datasets\\yolov8data\\valid\\images\\IMG-20200722-WA0008_jpg.rf.e75e3d9d3b772146d47b4864b0ce3149.jpg: 2 duplicate labels removed\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 17/17 2.8it/s 6.1s0.2s\n",
      "                   all        262      21716      0.696      0.632      0.646      0.442      0.539      0.475      0.454      0.216\n",
      "                I-beam         26        241      0.729      0.593      0.633      0.414      0.479      0.389       0.34      0.101\n",
      "                L-beam          4        171      0.635      0.789      0.754       0.46      0.113       0.14     0.0359    0.00743\n",
      "                O-beam         86       6870      0.916      0.966       0.97      0.705      0.908      0.957      0.962      0.531\n",
      "                O-pipe         28       2488      0.957      0.954      0.973      0.797      0.901      0.899      0.907      0.474\n",
      "                T-beam         26        367      0.771       0.39      0.414      0.328      0.638      0.322      0.316      0.135\n",
      "            square-bar         39       7129          0          0          0          0          0          0          0          0\n",
      "           square-pipe         54       4450      0.867       0.73      0.776      0.391      0.736      0.619      0.619      0.262\n",
      "Speed: 0.6ms preprocess, 4.2ms inference, 0.0ms loss, 3.9ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\UNSRI\\GDG\\steelbeam\\src\\runs\\segment\\val5\u001b[0m\n",
      "mAP50 (box) : 0.6459076889273158\n",
      "mAP50 (mask): 0.4541355285000173\n",
      "mAP50-95 (mask): 0.21578078270019396\n"
     ]
    }
   ],
   "source": [
    "best_model = YOLO(\"runs/segment/steelbeam-upgrade/weights/best.pt\") \n",
    "\n",
    "metrics = best_model.val(\n",
    "    data=DATA_YAML,\n",
    "    imgsz=320,\n",
    "    device=device,\n",
    "    split=\"val\"\n",
    ")\n",
    "\n",
    "print(\"mAP50 (box) :\", float(metrics.box.map50))\n",
    "print(\"mAP50 (mask):\", float(metrics.seg.map50))\n",
    "print(\"mAP50-95 (mask):\", float(metrics.seg.map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c873f739",
   "metadata": {},
   "source": [
    "### 2.4 Selecting the Best Model\n",
    "\n",
    "Several experimental runs were compared using different epochs, image sizes, and backbone variants.  \n",
    "Model selection was based on three criteria:\n",
    "\n",
    "1. **Segmentation Performance**  \n",
    "   The model with the most stable **mAP@0.5 (mask)** across validation images was prioritized.\n",
    "\n",
    "2. **Visual Mask Quality**  \n",
    "   Beyond numerical scores, visual results were examined to check:\n",
    "   - boundary sharpness  \n",
    "   - mask consistency across classes  \n",
    "   - ability to segment thin structures or stacked beams\n",
    "\n",
    "3. **Trade-off Between Precision and Speed**  \n",
    "   Since the model would later be deployed in a Streamlit app, inference speed and VRAM usage also influenced the final choice.  \n",
    "   Smaller backbones (like YOLOv8n-Seg) offered a good balance between segmentation quality and runtime efficiency.\n",
    "\n",
    "The final selected model was exported as **`best.pt`** and used for deployment in the next stage.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baae61a",
   "metadata": {},
   "source": [
    "## 3. Deployment with Streamlit\n",
    "\n",
    "To make the model easier to test and use, I deployed the best-performing model as a simple web application using **Streamlit**.\n",
    "\n",
    "The main steps were:\n",
    "\n",
    "1. Creating an `app.py` file inside the `src/` folder.  \n",
    "2. Inside `app.py`, I loaded the best checkpoint (`best.pt`) from  \n",
    "   `runs/segment/steelbeam-upgrade/weights/`.  \n",
    "3. The application provides several features:\n",
    "   - Uploading an image (JPG/PNG format)\n",
    "   - Displaying the original image on the left side\n",
    "   - Running YOLOv8-Seg inference in the background\n",
    "   - Showing the segmentation results (overlay mask + labels) on the right side\n",
    "   - Adjustable settings such as confidence threshold, IoU threshold, and image size\n",
    "\n",
    "How to run the app (it depends on your environment and path):\n",
    "\n",
    "```bash\n",
    "cd STEELBEAM\n",
    ".\\.venv\\Scripts\\activate      # activate the environment\n",
    "streamlit run src/app.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
